{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Assignment 3\n\n## Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\nA decision tree classifier is a popular machine learning algorithm used for type tasks. It works by partitioning the data into subsets or branches predicated on the values of input features. The algorithm makes prognostications by covering the tree from the root knot to a flake knot, where each internal knot represents a point and a split condition, and each flake knot represents a class marker. When a new data point is given, it follows the decision path down the tree, and the class marker associated with the flake knot it reaches becomes the prophecy.\n\n##Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\nDecision tree type is predicated on a set of if- fresh conditions. At each internal knot, the algorithm selects a point and a split condition to maximize information gain or Gini impurity reduction. These conditions are determined using fine criteria like entropy, Gini impurity, or misclassification error. The thing is to produce splits that separate the data into the purest subsets possible.\n\n##Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\nIn double type, the thing is to classify data points into one of two classes(e.g., Yes No, True/ False). A decision tree classifier creates a double tree structure, where each knot has two child bumps representing the double class labels. The splits made along the tree are predicated on point values, and the algorithm recursively partitions the data until it reaches pure subsets or a predefined stopping criterion.\n\n##Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions.\nWhile decision trees are generally imaged as a tree structure, there is also a geometric interpretation. At each knot, the algorithm creates a decision boundary that separates the point space into regions corresponding to different classes. Data points are classified predicated on the region of the point space they fall into. The geometric dubitation is that a decision tree defines a set of hyperplanes that partition the point space into regions associated with class labels.\n\n##Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.\nThe confusion matrix is a table that helps estimate the performance of a type model. It shows the factual and prognosticated groups of a model and consists of four values true cons( TP), true negatives( TN), false cons( FP), and false negatives( FN). From these values, various criteria like delicacy, perfection, recall, and F1 score can be calculated to assess the model's performance.\n\n##Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it.\nA confusion matrix may look like this:\n    Actual Class 0    Actual Class 1\nPredicted Class 0       100               20\nPredicted Class 1       10                50\nPrecision = TP / (TP + FP)\nRecall (Sensitivity) = TP / (TP + FN)\nF1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n\n##Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.\nThe choice of evaluation metric depends on the specific problem and the trade- offs between perfection and recall. For illustration, if false cons are precious, perfection may befurtherimportant.However, recall may be prioritized, If false negatives are precious. The choice of the metric should align with the business objects and the practical consequences of model errors.\n\n##Q8. Provide an example of a classification problem where precision is the most important metric, and explain why.\nAn example is a spam email classification problem. Precision is important here because classifying a legitimate email as spam (false positive) can lead to important emails being missed, which has significant consequences. Minimizing false positives (maximizing precision) is a priority.\n\n##Q9. Provide an example of a classification problem where recall is the most important metric and explain why.\nAn illustration is a medical opinion problem for a life- hanging complaint. In this case, recall is vital because missing a positive case( false negative) can have severe consequences. It's more important to ensure that all positive cases are detected, indeed if it means a advanced rate of false cons.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}